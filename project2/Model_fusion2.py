from matplotlib import rcParams
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
import lightgbm as lgb
import warnings
import os
from collections import Counter

warnings.filterwarnings('ignore')

# Windowsä¸­æ–‡å­—ä½“é…ç½®
rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
rcParams['axes.unicode_minus'] = False
rcParams['font.size'] = 12


class RegionalQuantileModel:
    """
    æ•´åˆå…­å¤§ç»æµåŒºåŸŸçš„åˆ†ä½æ•°å»ºæ¨¡ - ä¸“æ³¨95%åˆ†ä½æ•°ç‰ˆæœ¬
    """

    # å…­å¤§ç»æµåŒºåŸŸåˆ’åˆ†
    ECONOMIC_REGIONS = {
        'åä¸œ': ['31', '32', '33', '34', '35', '36', '37'],
        'ååŒ—': ['11', '12', '13', '14', '15'],
        'åä¸­': ['41', '42', '43'],
        'åå—': ['44', '45', '46'],
        'è¥¿å—': ['50', '51', '52', '53', '54'],
        'è¥¿åŒ—': ['61', '62', '63', '64', '65']
    }

    def __init__(self, train_path, test_path):
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        try:
            self.train_data = pd.read_excel(train_path)
            self.test_data = pd.read_excel(test_path)
            print(f"è®­ç»ƒé›†åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.train_data.shape}")
            print(f"æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.test_data.shape}")
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return

        # æ ¹æ®è¦æ±‚åˆ é™¤"å®šæŸç±»å‹"å’Œ"ä¸‰è€…äº¤é€šæ–¹å¼"ç‰¹å¾
        self.feature_names = [
            'ä¼¤åŠ¿ç¨‹åº¦', 'æ‰‹æœ¯æ¬¡æ•°ç»Ÿè®¡', 'æ²»ç–—æƒ…å†µ', 'è´£ä»»èµ”å¿ç³»æ•°',
            'éª¨æŠ˜ç±»æ•°é‡', 'æŠ¤ç†è´¹æ•°é‡', 'è½¯ç»„ç»‡æŸä¼¤ç±»æ•°é‡', 'é¢…è„‘æŸä¼¤ç±»æ•°é‡',
            'å†…è„æŸä¼¤ç±»æ•°é‡', 'ç¥ç»æŸä¼¤ç±»æ•°é‡', 'å…¶ä»–æŸä¼¤ç±»æ•°é‡', 'åœ°åŸŸ'
        ]
        self.models = {}
        self.scaler = StandardScaler()
        self.regional_stats = {}
        self.best_quantile = 0.95  # ä¸“æ³¨95%åˆ†ä½æ•°

    def prepare_data_with_regions(self):
        """
        å‡†å¤‡æ•°æ®å¹¶æ·»åŠ ç»æµåŒºåŸŸä¿¡æ¯
        """
        print("=== 1. æ•°æ®å‡†å¤‡ä¸åŒºåŸŸåˆ’åˆ† ===")

        # åŸºæœ¬æ•°æ®æ¸…æ´—
        train_df = self.train_data.copy()
        train_df = train_df[train_df['fee_all'] > 0]
        print(f"è®­ç»ƒé›†: {len(train_df)}æ¡")

        test_df = self.test_data.copy()
        test_df = test_df[test_df['fee_all'] > 0]
        print(f"æµ‹è¯•é›†: {len(test_df)}æ¡")

        # æ£€æŸ¥åœ°åŒºåˆ—
        for df in [train_df, test_df]:
            if 'åœ°åŸŸ' not in df.columns:
                print("è­¦å‘Š: æ•°æ®ä¸­ç¼ºå°‘'åœ°åŸŸ'åˆ—")
                df['åœ°åŸŸ'] = 'æœªçŸ¥åœ°åŒº'

            df['åœ°åŸŸ'] = df['åœ°åŸŸ'].astype(str)
            df['province_code'] = df['åœ°åŸŸ'].str[:2]

        print(f"è®­ç»ƒé›†æå–åˆ° {train_df['province_code'].nunique()} ä¸ªçœä»½")
        print(f"æµ‹è¯•é›†æå–åˆ° {test_df['province_code'].nunique()} ä¸ªçœä»½")

        # æ·»åŠ ç»æµåŒºåŸŸåˆ†ç±»
        train_df = self.add_economic_regions(train_df)
        test_df = self.add_economic_regions(test_df)

        # ç­›é€‰è®­ç»ƒé›†10-90%åŒºé—´
        train_df, lower_bound, upper_bound = self.filter_10_90_interval(train_df)

        # åˆ›å»ºä¸¤ç§æµ‹è¯•é›†ï¼š10-90%åŒºé—´å’Œå…¨éƒ¨æ•°æ®
        test_df_10_90 = test_df[
            (test_df['fee_all'] >= lower_bound) &
            (test_df['fee_all'] <= upper_bound)
            ].copy()

        print(f"æµ‹è¯•é›†10-90%åŒºé—´: æ ·æœ¬æ•°{len(test_df_10_90)}")
        print(f"æµ‹è¯•é›†å…¨éƒ¨æ•°æ®: æ ·æœ¬æ•°{len(test_df)}")

        return train_df, test_df_10_90, test_df, lower_bound, upper_bound

    def add_economic_regions(self, df):
        """æ·»åŠ ç»æµåŒºåŸŸåˆ†ç±»"""

        def get_economic_region(province_code):
            for region, codes in self.ECONOMIC_REGIONS.items():
                if province_code in codes:
                    return region
            return 'å…¶ä»–'

        df['economic_region'] = df['province_code'].map(get_economic_region)
        return df

    def filter_10_90_interval(self, df):
        """ç­›é€‰10-90%è´¹ç”¨åŒºé—´"""
        lower_bound = df['fee_all'].quantile(0.1)
        upper_bound = df['fee_all'].quantile(0.9)

        interval_data = df[
            (df['fee_all'] >= lower_bound) &
            (df['fee_all'] <= upper_bound)
            ].copy()

        print(f"10-90%åŒºé—´: è´¹ç”¨èŒƒå›´[{lower_bound:.0f}, {upper_bound:.0f}], æ ·æœ¬æ•°{len(interval_data)}")

        return interval_data, lower_bound, upper_bound

    def analyze_regional_distribution(self, train_df, test_df_10_90, test_df_all):
        """åˆ†æåœ°åŒºåˆ†å¸ƒ"""
        print("=== 2. åœ°åŒºåˆ†å¸ƒåˆ†æ ===")

        # è®­ç»ƒé›†ç»æµåŒºåŸŸç»Ÿè®¡
        train_region_stats = train_df.groupby('economic_region').agg({
            'fee_all': ['count', 'mean', 'median', 'std', 'min', 'max']
        }).round(2)

        train_region_stats.columns = ['æ ·æœ¬æ•°', 'å¹³å‡è´¹ç”¨', 'ä¸­ä½æ•°è´¹ç”¨', 'è´¹ç”¨æ ‡å‡†å·®', 'æœ€ä½è´¹ç”¨', 'æœ€é«˜è´¹ç”¨']
        train_region_stats = train_region_stats.sort_values('å¹³å‡è´¹ç”¨', ascending=False)

        self.regional_stats = train_region_stats

        print("è®­ç»ƒé›†ç»æµåŒºåŸŸè´¹ç”¨ç»Ÿè®¡:")
        for region, stats in train_region_stats.iterrows():
            print(f"  {region}: {int(stats['æ ·æœ¬æ•°'])}æ ·æœ¬, å¹³å‡è´¹ç”¨={stats['å¹³å‡è´¹ç”¨']:.2f}")

        # æµ‹è¯•é›†10-90%åŒºé—´ç»æµåŒºåŸŸç»Ÿè®¡
        test_10_90_stats = test_df_10_90.groupby('economic_region').agg({
            'fee_all': ['count', 'mean', 'median', 'std', 'min', 'max']
        }).round(2)

        test_10_90_stats.columns = ['æ ·æœ¬æ•°', 'å¹³å‡è´¹ç”¨', 'ä¸­ä½æ•°è´¹ç”¨', 'è´¹ç”¨æ ‡å‡†å·®', 'æœ€ä½è´¹ç”¨', 'æœ€é«˜è´¹ç”¨']
        test_10_90_stats = test_10_90_stats.sort_values('å¹³å‡è´¹ç”¨', ascending=False)

        print("\næµ‹è¯•é›†10-90%åŒºé—´ç»æµåŒºåŸŸè´¹ç”¨ç»Ÿè®¡:")
        for region, stats in test_10_90_stats.iterrows():
            print(f"  {region}: {int(stats['æ ·æœ¬æ•°'])}æ ·æœ¬, å¹³å‡è´¹ç”¨={stats['å¹³å‡è´¹ç”¨']:.2f}")

        # æµ‹è¯•é›†å…¨éƒ¨æ•°æ®ç»æµåŒºåŸŸç»Ÿè®¡
        test_all_stats = test_df_all.groupby('economic_region').agg({
            'fee_all': ['count', 'mean', 'median', 'std', 'min', 'max']
        }).round(2)

        test_all_stats.columns = ['æ ·æœ¬æ•°', 'å¹³å‡è´¹ç”¨', 'ä¸­ä½æ•°è´¹ç”¨', 'è´¹ç”¨æ ‡å‡†å·®', 'æœ€ä½è´¹ç”¨', 'æœ€é«˜è´¹ç”¨']
        test_all_stats = test_all_stats.sort_values('å¹³å‡è´¹ç”¨', ascending=False)

        print("\næµ‹è¯•é›†å…¨éƒ¨æ•°æ®ç»æµåŒºåŸŸè´¹ç”¨ç»Ÿè®¡:")
        for region, stats in test_all_stats.iterrows():
            print(f"  {region}: {int(stats['æ ·æœ¬æ•°'])}æ ·æœ¬, å¹³å‡è´¹ç”¨={stats['å¹³å‡è´¹ç”¨']:.2f}")

        self.visualize_regional_distribution(train_region_stats, test_10_90_stats, test_all_stats)

        return train_region_stats, test_10_90_stats, test_all_stats

    def visualize_regional_distribution(self, train_stats, test_10_90_stats, test_all_stats):
        """å¯è§†åŒ–åœ°åŒºåˆ†å¸ƒ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        regions = train_stats.index

        # 1. è®­ç»ƒé›†æ ·æœ¬é‡åˆ†å¸ƒ
        ax1.bar(regions, train_stats['æ ·æœ¬æ•°'].values, color='skyblue', alpha=0.7)
        ax1.set_title('è®­ç»ƒé›†å„ç»æµåŒºåŸŸæ ·æœ¬é‡')
        ax1.set_ylabel('æ ·æœ¬æ•°é‡')
        ax1.tick_params(axis='x', rotation=45)

        # 2. æµ‹è¯•é›†æ ·æœ¬é‡åˆ†å¸ƒ
        ax2.bar(regions, test_all_stats['æ ·æœ¬æ•°'].values, color='lightcoral', alpha=0.7)
        ax2.set_title('æµ‹è¯•é›†å„ç»æµåŒºåŸŸæ ·æœ¬é‡')
        ax2.set_ylabel('æ ·æœ¬æ•°é‡')
        ax2.tick_params(axis='x', rotation=45)

        # 3. è®­ç»ƒé›†å¹³å‡è´¹ç”¨åˆ†å¸ƒ
        ax3.bar(regions, train_stats['å¹³å‡è´¹ç”¨'].values, color='skyblue', alpha=0.7)
        ax3.set_title('è®­ç»ƒé›†å„ç»æµåŒºåŸŸå¹³å‡è´¹ç”¨')
        ax3.set_ylabel('å¹³å‡è´¹ç”¨')
        ax3.tick_params(axis='x', rotation=45)

        # 4. æµ‹è¯•é›†å¹³å‡è´¹ç”¨åˆ†å¸ƒ
        ax4.bar(regions, test_all_stats['å¹³å‡è´¹ç”¨'].values, color='lightcoral', alpha=0.7)
        ax4.set_title('æµ‹è¯•é›†å„ç»æµåŒºåŸŸå¹³å‡è´¹ç”¨')
        ax4.set_ylabel('å¹³å‡è´¹ç”¨')
        ax4.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.show()

    def create_regional_features(self, train_df, test_df):
        """åˆ›å»ºåœ°åŒºç‰¹å¾"""
        print("=== 3. åˆ›å»ºåœ°åŒºç‰¹å¾ ===")

        # åŸºç¡€ç‰¹å¾
        available_features = [f for f in self.feature_names if f in train_df.columns and f != 'åœ°åŸŸ']
        print(f"å¯ç”¨åŸºç¡€ç‰¹å¾: {available_features}")

        X_train = train_df[available_features].copy()
        y_train = train_df['fee_all']

        X_test = test_df[available_features].copy()
        y_test = test_df['fee_all']

        # å¤„ç†åˆ†ç±»å˜é‡
        categorical_cols = X_train.select_dtypes(include=['object']).columns
        self.label_encoders = {}
        for col in categorical_cols:
            print(f"å¤„ç†åˆ†ç±»å˜é‡: {col}")
            le = LabelEncoder()
            X_train[col] = le.fit_transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            self.label_encoders[col] = le

        # æ·»åŠ ç»æµåŒºåŸŸç‰¹å¾
        if 'economic_region' in train_df.columns:
            # ä½¿ç”¨è®­ç»ƒé›†è®¡ç®—åŒºåŸŸç»Ÿè®¡ç‰¹å¾
            region_means = train_df.groupby('economic_region')['fee_all'].mean()
            region_medians = train_df.groupby('economic_region')['fee_all'].median()
            region_std = train_df.groupby('economic_region')['fee_all'].std()
            region_counts = train_df.groupby('economic_region')['fee_all'].count()

            # ä¸ºè®­ç»ƒé›†æ·»åŠ åŒºåŸŸç‰¹å¾
            X_train['region_mean_cost'] = train_df['economic_region'].map(region_means)
            X_train['region_median_cost'] = train_df['economic_region'].map(region_medians)
            X_train['region_std_cost'] = train_df['economic_region'].map(region_std)
            X_train['region_sample_count'] = train_df['economic_region'].map(region_counts)

            # ä¸ºæµ‹è¯•é›†æ·»åŠ åŒºåŸŸç‰¹å¾
            X_test['region_mean_cost'] = test_df['economic_region'].map(region_means)
            X_test['region_median_cost'] = test_df['economic_region'].map(region_medians)
            X_test['region_std_cost'] = test_df['economic_region'].map(region_std)
            X_test['region_sample_count'] = test_df['economic_region'].map(region_counts)

            # å¤„ç†ç¼ºå¤±å€¼
            national_mean = y_train.mean()
            national_median = y_train.median()
            national_std = y_train.std()

            for df in [X_train, X_test]:
                df['region_mean_cost'].fillna(national_mean, inplace=True)
                df['region_median_cost'].fillna(national_median, inplace=True)
                df['region_std_cost'].fillna(national_std, inplace=True)
                df['region_sample_count'].fillna(0, inplace=True)

            # ç›¸å¯¹ç‰¹å¾
            X_train['cost_vs_region_mean'] = y_train / X_train['region_mean_cost']
            X_train['cost_vs_national_mean'] = y_train / national_mean

            X_test['cost_vs_region_mean'] = y_test / X_test['region_mean_cost']
            X_test['cost_vs_national_mean'] = y_test / national_mean

            # åŒºåŸŸç¼–ç 
            le_region = LabelEncoder()
            X_train['economic_region_encoded'] = le_region.fit_transform(train_df['economic_region'])
            X_test['economic_region_encoded'] = le_region.transform(test_df['economic_region'])

            # ä¿å­˜ç»Ÿè®¡é‡å’Œç¼–ç å™¨
            self.region_means = region_means
            self.region_medians = region_medians
            self.region_std = region_std
            self.region_counts = region_counts
            self.national_mean = national_mean
            self.national_median = national_median
            self.national_std = national_std
            self.region_encoder = le_region

            region_features = [col for col in X_train.columns if 'region' in col]
            print(f"æ–°å¢åœ°åŒºç‰¹å¾: {len(region_features)}ä¸ª")

        print(f"è®­ç»ƒé›†ç‰¹å¾æ•°é‡: {X_train.shape[1]}")
        print(f"æµ‹è¯•é›†ç‰¹å¾æ•°é‡: {X_test.shape[1]}")

        return X_train, X_test, y_train, y_test

    def train_quantile_model(self):
        """è®­ç»ƒ95%åˆ†ä½æ•°å›å½’æ¨¡å‹"""
        print("=== 4. è®­ç»ƒ95%åˆ†ä½æ•°å›å½’æ¨¡å‹ ===")

        # å‡†å¤‡æ•°æ®
        train_df, test_df_10_90, test_df_all, lower_bound, upper_bound = self.prepare_data_with_regions()

        if len(train_df) == 0:
            print("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return None

        self.train_df = train_df
        self.test_df_10_90 = test_df_10_90
        self.test_df_all = test_df_all

        # åˆ†æåœ°åŒºåˆ†å¸ƒ
        train_stats, test_10_90_stats, test_all_stats = self.analyze_regional_distribution(train_df, test_df_10_90,
                                                                                           test_df_all)

        # åˆ›å»ºç‰¹å¾
        X_train, X_test_10_90, y_train, y_test_10_90 = self.create_regional_features(train_df, test_df_10_90)
        _, X_test_all, _, y_test_all = self.create_regional_features(train_df, test_df_all)

        if X_train is None:
            print("é”™è¯¯: ç‰¹å¾åˆ›å»ºå¤±è´¥")
            return None

        # è®­ç»ƒ95%åˆ†ä½æ•°æ¨¡å‹
        X_train_scaled = self.scaler.fit_transform(X_train)

        print("è®­ç»ƒ 95% åˆ†ä½æ•°æ¨¡å‹...")
        try:
            model = lgb.LGBMRegressor(
                objective='quantile',
                alpha=0.95,  # 95%åˆ†ä½æ•°
                n_estimators=1000,
                learning_rate=0.05,
                max_depth=6,
                random_state=42,
                verbose=-1
            )

            model.fit(X_train_scaled, y_train)

            self.models[0.95] = {
                'model': model,
                'metrics': {}
            }

            print("  95%åˆ†ä½æ•°æ¨¡å‹è®­ç»ƒå®Œæˆ")

            # è¯„ä¼°æ¨¡å‹
            self.evaluate_model(X_test_10_90, y_test_10_90, "æµ‹è¯•é›†10-90%åŒºé—´")
            self.evaluate_model(X_test_all, y_test_all, "æµ‹è¯•é›†å…¨éƒ¨æ•°æ®")

            # å¯è§†åŒ–ç»“æœ
            self.visualize_results()

            return self.models

        except Exception as e:
            print(f"  è®­ç»ƒå¤±è´¥: {e}")
            return None

    def evaluate_model(self, X_test, y_test, test_set_name="æµ‹è¯•é›†"):
        """è¯„ä¼°95%åˆ†ä½æ•°æ¨¡å‹åœ¨ç‰¹å®šæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        print(f"=== 5. åœ¨{test_set_name}ä¸Šè¯„ä¼°æ¨¡å‹ ===")

        if 0.95 not in self.models:
            print("æ²¡æœ‰å¯è¯„ä¼°çš„æ¨¡å‹")
            return {}

        X_test_scaled = self.scaler.transform(X_test)
        model_info = self.models[0.95]

        print(f"è¯„ä¼° 95% åˆ†ä½æ•°æ¨¡å‹åœ¨{test_set_name}ä¸Šçš„æ€§èƒ½...")

        try:
            model = model_info['model']
            y_pred = model.predict(X_test_scaled)

            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            r2 = r2_score(y_test, y_pred)
            relative_error = mae / y_test.mean() if y_test.mean() > 0 else 0

            def quantile_loss(y_true, y_pred, q):
                error = y_true - y_pred
                return np.maximum(q * error, (q - 1) * error).mean()

            q_loss = quantile_loss(y_test, y_pred, 0.95)

            model_info['metrics'][test_set_name] = {
                'MAE': mae,
                'RMSE': rmse,
                'R2': r2,
                'Relative_Error': relative_error,
                'Quantile_Loss': q_loss,
                'predictions': y_pred,
                'actuals': y_test
            }

            print(f"  95%åˆ†ä½æ•° - RÂ²: {r2:.4f}, MAE: {mae:.2f}, "
                  f"ç›¸å¯¹è¯¯å·®: {relative_error:.2%}")

            return model_info['metrics'][test_set_name]

        except Exception as e:
            print(f"  è¯„ä¼°å¤±è´¥: {e}")
            return {}

    def visualize_results(self):
        """å¯è§†åŒ–95%åˆ†ä½æ•°æ¨¡å‹ç»“æœ"""
        print("=== 6. å¯è§†åŒ–ç»“æœ ===")

        if 0.95 not in self.models:
            print("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç»“æœè¿›è¡Œå¯è§†åŒ–")
            return

        model_info = self.models[0.95]

        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

            # 1. æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
            metrics_10_90 = model_info['metrics']['æµ‹è¯•é›†10-90%åŒºé—´']
            metrics_all = model_info['metrics']['æµ‹è¯•é›†å…¨éƒ¨æ•°æ®']

            metrics_names = ['RÂ²', 'MAE', 'ç›¸å¯¹è¯¯å·®']
            metrics_10_90_values = [metrics_10_90['R2'], metrics_10_90['MAE'], metrics_10_90['Relative_Error']]
            metrics_all_values = [metrics_all['R2'], metrics_all['MAE'], metrics_all['Relative_Error']]

            x = np.arange(len(metrics_names))
            width = 0.35

            bars1 = ax1.bar(x - width / 2, metrics_10_90_values, width,
                            label='10-90%åŒºé—´', color='lightblue', alpha=0.7)
            bars2 = ax1.bar(x + width / 2, metrics_all_values, width,
                            label='å…¨éƒ¨æ•°æ®', color='lightcoral', alpha=0.7)

            ax1.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
            ax1.set_ylabel('æ•°å€¼')
            ax1.set_title('95%åˆ†ä½æ•°æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ')
            ax1.set_xticks(x)
            ax1.set_xticklabels(metrics_names)
            ax1.legend()

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars1, metrics_10_90_values):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                         f'{value:.3f}' if value > 0.01 else f'{value:.3%}',
                         ha='center', va='bottom', fontsize=9)

            for bar, value in zip(bars2, metrics_all_values):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                         f'{value:.3f}' if value > 0.01 else f'{value:.3%}',
                         ha='center', va='bottom', fontsize=9)

            # 2. 10-90%åŒºé—´é¢„æµ‹æ•ˆæœæ•£ç‚¹å›¾
            y_pred_10_90 = metrics_10_90['predictions']
            y_test_10_90 = metrics_10_90['actuals']

            ax2.scatter(y_test_10_90, y_pred_10_90, alpha=0.5, color='blue', s=20)
            max_val = max(y_test_10_90.max(), y_pred_10_90.max())
            ax2.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='ç†æƒ³é¢„æµ‹çº¿')
            ax2.set_xlabel('å®é™…è´¹ç”¨')
            ax2.set_ylabel('é¢„æµ‹è´¹ç”¨')
            ax2.set_title('95%åˆ†ä½æ•°æ¨¡å‹åœ¨10-90%åŒºé—´çš„é¢„æµ‹æ•ˆæœ')
            ax2.legend()

            # 3. å…¨éƒ¨æ•°æ®é¢„æµ‹æ•ˆæœæ•£ç‚¹å›¾
            y_pred_all = metrics_all['predictions']
            y_test_all = metrics_all['actuals']

            ax3.scatter(y_test_all, y_pred_all, alpha=0.5, color='red', s=20)
            max_val = max(y_test_all.max(), y_pred_all.max())
            ax3.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='ç†æƒ³é¢„æµ‹çº¿')
            ax3.set_xlabel('å®é™…è´¹ç”¨')
            ax3.set_ylabel('é¢„æµ‹è´¹ç”¨')
            ax3.set_title('95%åˆ†ä½æ•°æ¨¡å‹åœ¨å…¨éƒ¨æ•°æ®ä¸Šçš„é¢„æµ‹æ•ˆæœ')
            ax3.legend()

            # 4. æ®‹å·®åˆ†å¸ƒå›¾
            residuals_10_90 = y_test_10_90 - y_pred_10_90
            residuals_all = y_test_all - y_pred_all

            ax4.hist(residuals_10_90, bins=50, alpha=0.7, label='10-90%åŒºé—´', color='lightblue')
            ax4.hist(residuals_all, bins=50, alpha=0.7, label='å…¨éƒ¨æ•°æ®', color='lightcoral')
            ax4.set_xlabel('æ®‹å·®')
            ax4.set_ylabel('é¢‘æ•°')
            ax4.set_title('95%åˆ†ä½æ•°æ¨¡å‹æ®‹å·®åˆ†å¸ƒ')
            ax4.legend()

            plt.tight_layout()
            plt.show()

            # ä¸šåŠ¡å»ºè®®
            self.business_recommendations()

        except Exception as e:
            print(f"å¯è§†åŒ–å¤±è´¥: {e}")

    def business_recommendations(self):
        """ä¸šåŠ¡å»ºè®®"""
        print("=== 7. ä¸šåŠ¡å»ºè®® ===")

        if 0.95 not in self.models:
            print("æ²¡æœ‰å¯ç”¨çš„ç»“æœè¿›è¡Œä¸šåŠ¡å»ºè®®")
            return

        model_info = self.models[0.95]
        metrics_10_90 = model_info['metrics'].get('æµ‹è¯•é›†10-90%åŒºé—´', {})
        metrics_all = model_info['metrics'].get('æµ‹è¯•é›†å…¨éƒ¨æ•°æ®', {})

        print("æ¨èä½¿ç”¨åˆ†ä½æ•°: 95%")

        if metrics_10_90:
            print(f"\næµ‹è¯•é›†10-90%åŒºé—´æ€§èƒ½:")
            print(f"  RÂ²={metrics_10_90['R2']:.4f}, MAE={metrics_10_90['MAE']:.2f}, "
                  f"ç›¸å¯¹è¯¯å·®={metrics_10_90['Relative_Error']:.2%}")

        if metrics_all:
            print(f"\næµ‹è¯•é›†å…¨éƒ¨æ•°æ®æ€§èƒ½:")
            print(f"  RÂ²={metrics_all['R2']:.4f}, MAE={metrics_all['MAE']:.2f}, "
                  f"ç›¸å¯¹è¯¯å·®={metrics_all['Relative_Error']:.2%}")

        best_r2 = max(metrics_10_90.get('R2', 0), metrics_all.get('R2', 0))

        if best_r2 > 0.7:
            print("\nâœ… æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºä¸šåŠ¡é¢„æµ‹")
        elif best_r2 > 0.6:
            print("\nâœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä½œä¸ºé‡è¦å‚è€ƒ")
        elif best_r2 > 0.5:
            print("\nâš ï¸ æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®ç»“åˆä¸šåŠ¡ç»éªŒä½¿ç”¨")
        else:
            print("\nâŒ æ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼Œä¸»è¦ç”¨äºæ•°æ®æ¢ç´¢")

        print(f"\nğŸ’¡ğŸ’¡ ä¸šåŠ¡è§£è¯»:")
        print(f"  â€¢ 95%åˆ†ä½æ•°æ¨¡å‹é€‚ç”¨äºé«˜é£é™©æ¡ˆä»¶çš„è´¹ç”¨é¢„ä¼°")
        print(f"  â€¢ æ¨¡å‹åœ¨æ­£å¸¸è´¹ç”¨åŒºé—´(10-90%)è¡¨ç°ä¼˜å¼‚ï¼ŒRÂ²è¾¾åˆ°{metrics_10_90.get('R2', 0):.4f}")
        print(f"  â€¢ åœ¨å…¨éƒ¨æ•°æ®ä¸Šç›¸å¯¹è¯¯å·®ä¸º{metrics_all.get('Relative_Error', 0):.2%}")

        return {
            'best_quantile': 0.95,
            'metrics_10_90': metrics_10_90,
            'metrics_all': metrics_all
        }

    def run_analysis(self):
        """è¿è¡Œåˆ†ææµç¨‹"""
        print("å¼€å§‹95%åˆ†ä½æ•°å»ºæ¨¡åˆ†æ")
        print("=" * 60)

        try:
            # è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°
            models = self.train_quantile_model()

            if models is not None:
                print("\nâœ… åˆ†æå®Œæˆ!")
                return {
                    'models': self.models,
                    'best_quantile': 0.95
                }
            else:
                print("âŒ åˆ†æå¤±è´¥")
                return None

        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None


class CasePredictionSystem:
    """æ¡ˆä»¶è´¹ç”¨é¢„æµ‹äº¤äº’ç³»ç»Ÿ"""

    def __init__(self, regional_model):
        self.model = regional_model
        self.feature_mappings = self._create_feature_mappings()

    def _create_feature_mappings(self):
        """åˆ›å»ºç‰¹å¾æ˜ å°„å­—å…¸"""
        mappings = {
            'ä¼¤åŠ¿ç¨‹åº¦': {
                'ä¼¤': 1, 'æ®‹': 2, 'æ­»äº¡': 3
            },
            'æ²»ç–—æƒ…å†µ': {
                'é—¨è¯Šæ²»ç–—': 1, 'ä½é™¢æ²»ç–—': 2, 'å½“åœºæ­»äº¡': 3
            }
        }
        return mappings

    def get_case_input(self):
        """äº¤äº’å¼è·å–æ¡ˆä»¶ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ§‘âš•ï¸ åŒ»ç–—æ¡ˆä»¶è´¹ç”¨é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 60)

        case_info = {}

        # 1. ä¼¤åŠ¿ç¨‹åº¦
        print("\nğŸ“Š ä¼¤åŠ¿ç¨‹åº¦é€‰æ‹©:")
        for level, code in self.feature_mappings['ä¼¤åŠ¿ç¨‹åº¦'].items():
            print(f"  {code}: {level}")
        injury_level = input("è¯·é€‰æ‹©ä¼¤åŠ¿ç¨‹åº¦ç¼–å·(1-3): ").strip()
        case_info['ä¼¤åŠ¿ç¨‹åº¦'] = int(injury_level) if injury_level.isdigit() and 1 <= int(injury_level) <= 3 else 1

        # 2. è´£ä»»ç³»æ•°
        liability = input("\nâš–ï¸ è´£ä»»èµ”å¿ç³»æ•°(0-1ä¹‹é—´ï¼Œå¦‚0.7è¡¨ç¤º70%è´£ä»»): ").strip()
        case_info['è´£ä»»èµ”å¿ç³»æ•°'] = float(liability) if liability.replace('.', '').isdigit() else 0.5

        # 3. æ‰‹æœ¯æ¬¡æ•°
        surgery_count = input("\nğŸ¥ æ‰‹æœ¯æ¬¡æ•°: ").strip()
        case_info['æ‰‹æœ¯æ¬¡æ•°ç»Ÿè®¡'] = int(surgery_count) if surgery_count.isdigit() else 0

        # 4. æ²»ç–—æƒ…å†µ
        print("\nğŸ’Š æ²»ç–—æƒ…å†µé€‰æ‹©:")
        for treatment, code in self.feature_mappings['æ²»ç–—æƒ…å†µ'].items():
            print(f"  {code}: {treatment}")
        treatment_code = input("è¯·é€‰æ‹©æ²»ç–—æƒ…å†µç¼–å·(1-3): ").strip()
        case_info['æ²»ç–—æƒ…å†µ'] = int(treatment_code) if treatment_code.isdigit() and 1 <= int(treatment_code) <= 3 else 2

        # 5. æŸä¼¤æ•°é‡è¾“å…¥ - è°ƒæ•´é¡ºåºï¼Œå°†å…¶ä»–æŸä¼¤ç±»æ•°é‡æ”¾åœ¨æœ€å
        print("\nğŸ¤• æŸä¼¤æ•°é‡è¾“å…¥:")
        injury_types = [
            'éª¨æŠ˜ç±»æ•°é‡', 'è½¯ç»„ç»‡æŸä¼¤ç±»æ•°é‡', 'é¢…è„‘æŸä¼¤ç±»æ•°é‡',
            'å†…è„æŸä¼¤ç±»æ•°é‡', 'ç¥ç»æŸä¼¤ç±»æ•°é‡', 'å…¶ä»–æŸä¼¤ç±»æ•°é‡'
        ]

        for injury_type in injury_types:
            count = input(f"{injury_type}: ").strip()
            case_info[injury_type] = int(count) if count.isdigit() else 0

        # 6. æŠ¤ç†è´¹æ•°é‡
        nursing_fee = input("\nğŸ©º æŠ¤ç†è´¹æ•°é‡(æŠ¤ç†å¤©æ•°): ").strip()
        case_info['æŠ¤ç†è´¹æ•°é‡'] = int(nursing_fee) if nursing_fee.isdigit() else 0

        # 7. åœ°åŸŸä¿¡æ¯
        print("\nğŸŒ åœ°åŸŸä¿¡æ¯:")
        province_code = input("è¯·è¾“å…¥çœä»½ä»£ç (å¦‚31-ä¸Šæµ·, 44-å¹¿ä¸œç­‰): ").strip()
        case_info['åœ°åŸŸ'] = province_code if province_code else '31'

        return case_info

    def preprocess_case_data(self, case_info):
        """é¢„å¤„ç†æ¡ˆä»¶æ•°æ®"""
        # åˆ›å»ºåŸºç¡€ç‰¹å¾DataFrame
        base_features = [
            'ä¼¤åŠ¿ç¨‹åº¦', 'æ‰‹æœ¯æ¬¡æ•°ç»Ÿè®¡', 'æ²»ç–—æƒ…å†µ', 'è´£ä»»èµ”å¿ç³»æ•°',
            'éª¨æŠ˜ç±»æ•°é‡', 'æŠ¤ç†è´¹æ•°é‡', 'è½¯ç»„ç»‡æŸä¼¤ç±»æ•°é‡', 'é¢…è„‘æŸä¼¤ç±»æ•°é‡',
            'å†…è„æŸä¼¤ç±»æ•°é‡', 'ç¥ç»æŸä¼¤ç±»æ•°é‡', 'å…¶ä»–æŸä¼¤ç±»æ•°é‡', 'åœ°åŸŸ'
        ]

        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
        for feature in base_features:
            if feature not in case_info:
                case_info[feature] = 0

        # åˆ›å»ºDataFrame
        case_df = pd.DataFrame([case_info])

        # ç¡®ä¿province_codeåˆ—å­˜åœ¨
        if 'province_code' not in case_df.columns and 'åœ°åŸŸ' in case_df.columns:
            case_df['province_code'] = case_df['åœ°åŸŸ'].astype(str).str[:2]

        # æ·»åŠ ç»æµåŒºåŸŸä¿¡æ¯
        case_df = self.model.add_economic_regions(case_df)

        # æå–ç‰¹å¾
        available_features = [f for f in self.model.feature_names if f in case_df.columns and f != 'åœ°åŸŸ']
        X_case = case_df[available_features].copy()

        # å¤„ç†åˆ†ç±»å˜é‡
        categorical_cols = X_case.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if hasattr(self.model, 'label_encoders') and col in self.model.label_encoders:
                le = self.model.label_encoders[col]
                X_case[col] = le.transform(X_case[col].astype(str))
            else:
                X_case[col] = X_case[col].astype(str).astype('category').cat.codes

        # æ·»åŠ åŒºåŸŸç‰¹å¾
        if hasattr(self.model, 'region_means') and 'economic_region' in case_df.columns:
            region = case_df['economic_region'].iloc[0]

            X_case['region_mean_cost'] = self.model.region_means.get(region,
                                                                     getattr(self.model, 'national_mean', 10000))
            X_case['region_median_cost'] = self.model.region_medians.get(region,
                                                                         getattr(self.model, 'national_median', 8000))
            X_case['region_std_cost'] = self.model.region_std.get(region, getattr(self.model, 'national_std', 3000))
            X_case['region_sample_count'] = self.model.region_counts.get(region, 0)

            national_mean = getattr(self.model, 'national_mean', 10000)
            X_case['cost_vs_region_mean'] = national_mean / X_case['region_mean_cost']
            X_case['cost_vs_national_mean'] = 1.0

            # åŒºåŸŸç¼–ç 
            if hasattr(self.model, 'region_encoder'):
                try:
                    X_case['economic_region_encoded'] = self.model.region_encoder.transform([region])[0]
                except:
                    X_case['economic_region_encoded'] = 0

        return X_case

    def predict_case_cost(self, case_info=None):
        """é¢„æµ‹æ¡ˆä»¶è´¹ç”¨"""
        if case_info is None:
            case_info = self.get_case_input()

        print("\n" + "=" * 60)
        print("ğŸ”® æ­£åœ¨è¿›è¡Œè´¹ç”¨é¢„æµ‹...")
        print("=" * 60)

        try:
            # é¢„å¤„ç†æ¡ˆä»¶æ•°æ®
            X_case = self.preprocess_case_data(case_info)

            # ç‰¹å¾æ ‡å‡†åŒ–
            X_case_scaled = self.model.scaler.transform(X_case)

            # ä½¿ç”¨95%åˆ†ä½æ•°æ¨¡å‹è¿›è¡Œé¢„æµ‹
            if 0.95 in self.model.models:
                model = self.model.models[0.95]['model']
                prediction = model.predict(X_case_scaled)[0]
                predictions = {'95%åˆ†ä½æ•°': round(prediction, 2)}
            else:
                print("âŒ 95%åˆ†ä½æ•°æ¨¡å‹æœªè®­ç»ƒ")
                return None

            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            predicted_value = self.display_prediction_results(case_info, predictions)

            return predictions, predicted_value

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def display_prediction_results(self, case_info, predictions):
        """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
        print("\nğŸ¯ é¢„æµ‹ç»“æœ:")
        print("=" * 40)

        # æ˜¾ç¤ºè¾“å…¥ä¿¡æ¯æ‘˜è¦
        print("ğŸ“‹ æ¡ˆä»¶ä¿¡æ¯æ‘˜è¦:")
        print(f"  ä¼¤åŠ¿ç¨‹åº¦: {self._get_key_by_value(self.feature_mappings['ä¼¤åŠ¿ç¨‹åº¦'], case_info.get('ä¼¤åŠ¿ç¨‹åº¦', 1))}")
        print(f"  è´£ä»»ç³»æ•°: {case_info.get('è´£ä»»èµ”å¿ç³»æ•°', 'æœªçŸ¥')}")
        print(f"  æ‰‹æœ¯æ¬¡æ•°: {case_info.get('æ‰‹æœ¯æ¬¡æ•°ç»Ÿè®¡', 0)}")
        print(f"  æ²»ç–—æƒ…å†µ: {self._get_key_by_value(self.feature_mappings['æ²»ç–—æƒ…å†µ'], case_info.get('æ²»ç–—æƒ…å†µ', 2))}")

        # æ˜¾ç¤ºæŸä¼¤æƒ…å†µ
        injury_features = ['éª¨æŠ˜ç±»æ•°é‡', 'è½¯ç»„ç»‡æŸä¼¤ç±»æ•°é‡', 'é¢…è„‘æŸä¼¤ç±»æ•°é‡',
                           'å†…è„æŸä¼¤ç±»æ•°é‡', 'ç¥ç»æŸä¼¤ç±»æ•°é‡', 'å…¶ä»–æŸä¼¤ç±»æ•°é‡']
        injury_summary = []
        for feature in injury_features:
            count = case_info.get(feature, 0)
            if count > 0:
                injury_summary.append(f"{feature}: {count}")

        if injury_summary:
            print(f"  æŸä¼¤æƒ…å†µ: {', '.join(injury_summary)}")

        # æ˜¾ç¤ºé¢„æµ‹è´¹ç”¨
        print("\nğŸ’° è´¹ç”¨é¢„æµ‹ç»“æœ:")
        for quantile, cost in predictions.items():
            print(f"  {quantile}: Â¥{cost:,.2f}")

        # ä¸šåŠ¡è§£è¯»
        print("\nğŸ’¡ ä¸šåŠ¡è§£è¯»:")
        if '95%åˆ†ä½æ•°' in predictions:
            high_cost = predictions['95%åˆ†ä½æ•°']
            print(f"  â€¢ é£é™©é¢„ä¼°(95%åˆ†ä½æ•°): Â¥{high_cost:,.2f}")
            print(f"  â€¢ æ­¤é¢„ä¼°è€ƒè™‘äº†é«˜é£é™©æƒ…å†µï¼Œé€‚åˆä½œä¸ºæœ€é«˜è´¹ç”¨å‚è€ƒ")

        print("=" * 40)

        # è¿”å›é¢„æµ‹å€¼ç”¨äºå¼‚å¸¸æ£€æµ‹
        return high_cost if '95%åˆ†ä½æ•°' in predictions else None

    def _get_key_by_value(self, dictionary, value):
        """æ ¹æ®å€¼è·å–å­—å…¸ä¸­çš„é”®"""
        for key, val in dictionary.items():
            if val == value:
                return key
        return "æœªçŸ¥"


class BoxplotAnomalyDetector:
    def __init__(self, file_path=None, data_series=None, column_name='fee_all'):
        """
        åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
        :param file_path: Excelæ–‡ä»¶è·¯å¾„
        :param data_series: æ•°æ®åºåˆ— (DataFrameåˆ—)
        :param column_name: è¦åˆ†æçš„åˆ—å
        """
        self.file_path = file_path
        self.column_name = column_name
        self.data = None
        self.percentile_95 = None
        self.yellow_threshold = None
        self.orange_threshold = None
        self.red_threshold = None

        # æµ‹è¯•æ•°æ®è®¡æ•°å™¨
        self.test_count = 0
        self.test_data_list = []

        # è¯»å–æ•°æ®
        if not self.load_data(file_path, data_series):
            return

        # è®¡ç®—ç»Ÿè®¡é‡
        self.calculate_statistics()

    def load_data(self, file_path, data_series):
        """è¯»å–æ•°æ® - æ”¯æŒæ–‡ä»¶è·¯å¾„æˆ–æ•°æ®åºåˆ—"""
        try:
            if data_series is not None:
                # ä½¿ç”¨æä¾›çš„æ•°æ®åºåˆ—
                self.data = data_series.dropna()
                print(f"æˆåŠŸè¯»å–æ•°æ®åºåˆ—ï¼Œå½¢çŠ¶: {len(self.data)}")
                return True
            elif file_path is not None and os.path.exists(file_path):
                # ä»æ–‡ä»¶è¯»å–æ•°æ®
                df = pd.read_excel(file_path)

                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                if self.column_name not in df.columns:
                    available_columns = ", ".join(df.columns)
                    print(f"é”™è¯¯: åˆ— '{self.column_name}' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")
                    print(f"å¯ç”¨çš„åˆ—æœ‰: {available_columns}")
                    return False

                self.data = df[self.column_name].dropna()
                print(f"æˆåŠŸä»æ–‡ä»¶è¯»å– {len(self.data)} æ¡æ•°æ®")
                return True
            else:
                print("é”™è¯¯: æ²¡æœ‰æä¾›æœ‰æ•ˆçš„æ•°æ®æº")
                return False

        except Exception as e:
            print(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def calculate_statistics(self):
        """è®¡ç®—ç™¾åˆ†ä½æ•°å’Œé¢„è­¦é˜ˆå€¼"""
        try:
            # è®¡ç®—95%åˆ†ä½æ•°
            self.percentile_95 = self.data.quantile(0.95)

            # ä¿®æ­£é˜ˆå€¼è®¡ç®—é€»è¾‘ - ç›´æ¥åŸºäº95%åˆ†ä½æ•°çš„å€æ•°
            self.yellow_threshold = self.percentile_95 * 1.1  # 10%å¢å¹…
            self.orange_threshold = self.percentile_95 * 1.3  # 30%å¢å¹…
            self.red_threshold = self.percentile_95 * 1.5  # 50%å¢å¹…

            print("\n=== å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡é‡è®¡ç®—å®Œæˆ ===")
            print(f"95%åˆ†ä½æ•°: {self.percentile_95:.2f}")
            print(f"é»„è‰²é¢„è­¦é˜ˆå€¼ (95%åˆ†ä½æ•°çš„1.1å€): {self.yellow_threshold:.2f}")
            print(f"æ©™è‰²é¢„è­¦é˜ˆå€¼ (95%åˆ†ä½æ•°çš„1.3å€): {self.orange_threshold:.2f}")
            print(f"çº¢è‰²é¢„è­¦é˜ˆå€¼ (95%åˆ†ä½æ•°çš„1.5å€): {self.red_threshold:.2f}")

        except Exception as e:
            print(f"è®¡ç®—ç»Ÿè®¡é‡æ—¶å‡ºé”™: {e}")

    def update_statistics(self):
        """å½“æµ‹è¯•æ•°æ®è¾¾åˆ°100æ¬¡æ—¶æ›´æ–°ç»Ÿè®¡é‡"""
        try:
            if len(self.test_data_list) >= 100:
                print(f"\næµ‹è¯•æ•°æ®å·²è¾¾åˆ° {len(self.test_data_list)} æ¡ï¼Œæ›´æ–°ç»Ÿè®¡é‡...")

                # åˆå¹¶åŸå§‹æ•°æ®å’Œæµ‹è¯•æ•°æ®
                combined_data = pd.concat([self.data, pd.Series(self.test_data_list)])

                # é‡æ–°è®¡ç®—ç»Ÿè®¡é‡
                self.percentile_95 = combined_data.quantile(0.95)
                self.yellow_threshold = self.percentile_95 * 1.1
                self.orange_threshold = self.percentile_95 * 1.3
                self.red_threshold = self.percentile_95 * 1.5

                print("=== ç»Ÿè®¡é‡å·²æ›´æ–° ===")
                print(f"æ–°çš„95%åˆ†ä½æ•°: {self.percentile_95:.2f}")
                print(f"æ–°çš„é»„è‰²é¢„è­¦é˜ˆå€¼: {self.yellow_threshold:.2f}")
                print(f"æ–°çš„æ©™è‰²é¢„è­¦é˜ˆå€¼: {self.orange_threshold:.2f}")
                print(f"æ–°çš„çº¢è‰²é¢„è­¦é˜ˆå€¼: {self.red_threshold:.2f}")

                # é‡ç½®è®¡æ•°å™¨
                self.test_data_list = []
                self.test_count = 0

                return True
            return False
        except Exception as e:
            print(f"æ›´æ–°ç»Ÿè®¡é‡æ—¶å‡ºé”™: {e}")
            return False

    def is_initialized(self):
        """æ£€æŸ¥æ£€æµ‹å™¨æ˜¯å¦æˆåŠŸåˆå§‹åŒ–"""
        return self.data is not None and self.percentile_95 is not None

    def detect_anomaly(self, value):
        """
        æ£€æµ‹å•ä¸ªå€¼æ˜¯å¦ä¸ºå¼‚å¸¸å€¼
        :param value: è¦æ£€æµ‹çš„æ•°å€¼
        :return: æ£€æµ‹ç»“æœå­—ç¬¦ä¸²å’Œå¼‚å¸¸ç±»å‹
        """
        if not self.is_initialized():
            return "é”™è¯¯: æ£€æµ‹å™¨æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹", "error"

        try:
            value = float(value)

            # è®°å½•æµ‹è¯•æ•°æ®
            self.test_count += 1
            self.test_data_list.append(value)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç»Ÿè®¡é‡
            self.update_statistics()

            # æ ¹æ®æ–°çš„é¢„è­¦è§„åˆ™è¿›è¡Œåˆ†ç±»
            if value > self.red_threshold:
                return f"å€¼ {value:,.2f} å±äº **çº¢è‰²é¢„è­¦** (è¶…è¿‡95%åˆ†ä½æ•°50%ä»¥ä¸Š)", "red"
            elif value > self.orange_threshold:
                return f"å€¼ {value:,.2f} å±äº **æ©™è‰²é¢„è­¦** (è¶…è¿‡95%åˆ†ä½æ•°30%-50%)", "orange"
            elif value > self.yellow_threshold:
                return f"å€¼ {value:,.2f} å±äº **é»„è‰²é¢„è­¦** (è¶…è¿‡95%åˆ†ä½æ•°10%-30%)", "yellow"
            elif value > self.percentile_95:
                return f"å€¼ {value:,.2f} å±äº **è½»å¾®è¶…å‡º** (è¶…è¿‡95%åˆ†ä½æ•°ä½†åœ¨10%ä»¥å†…)", "slight"
            else:
                return f"å€¼ {value:,.2f} å±äº **æ­£å¸¸èŒƒå›´**", "normal"

        except ValueError:
            return "é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å€¼", "error"

    def classify_anomaly_category(self, value):
        """
        åˆ†ç±»å¼‚å¸¸ç±»åˆ«ï¼ˆè¿”å›ç±»åˆ«ç¼–å·ï¼‰
        :param value: è¦æ£€æµ‹çš„æ•°å€¼
        :return: å¼‚å¸¸ç±»åˆ«ç¼–å· (0:æ­£å¸¸, 1:è½»å¾®è¶…å‡º, 2:é»„è‰²é¢„è­¦, 3:æ©™è‰²é¢„è­¦, 4:çº¢è‰²é¢„è­¦)
        """
        if not self.is_initialized():
            return -1  # é”™è¯¯ä»£ç 

        try:
            value = float(value)

            if value > self.red_threshold:
                return 4  # çº¢è‰²é¢„è­¦
            elif value > self.orange_threshold:
                return 3  # æ©™è‰²é¢„è­¦
            elif value > self.yellow_threshold:
                return 2  # é»„è‰²é¢„è­¦
            elif value > self.percentile_95:
                return 1  # è½»å¾®è¶…å‡º
            else:
                return 0  # æ­£å¸¸èŒƒå›´

        except ValueError:
            return -1  # é”™è¯¯ä»£ç 

    def print_warning_stats(self):
        """æ‰“å°é¢„è­¦ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_initialized():
            return

        try:
            # è®¡ç®—å„é¢„è­¦çº§åˆ«çš„æ•°é‡
            normal = self.data[self.data <= self.percentile_95]
            slight_outliers = self.data[(self.data > self.percentile_95) & (self.data <= self.yellow_threshold)]
            yellow_warnings = self.data[(self.data > self.yellow_threshold) & (self.data <= self.orange_threshold)]
            orange_warnings = self.data[(self.data > self.orange_threshold) & (self.data <= self.red_threshold)]
            red_warnings = self.data[self.data > self.red_threshold]

            print("\n=== å¼‚å¸¸æ£€æµ‹é¢„è­¦ç»Ÿè®¡ ===")
            print(f"æ•°æ®æ€»é‡: {len(self.data)}")
            print(f"95%åˆ†ä½æ•°: {self.percentile_95:.2f}")
            print(f"æ­£å¸¸èŒƒå›´æ•°é‡: {len(normal)} ({len(normal) / len(self.data) * 100:.2f}%)")
            print(f"è½»å¾®è¶…å‡ºæ•°é‡: {len(slight_outliers)} ({len(slight_outliers) / len(self.data) * 100:.2f}%)")
            print(f"é»„è‰²é¢„è­¦æ•°é‡: {len(yellow_warnings)} ({len(yellow_warnings) / len(self.data) * 100:.2f}%)")
            print(f"æ©™è‰²é¢„è­¦æ•°é‡: {len(orange_warnings)} ({len(orange_warnings) / len(self.data) * 100:.2f}%)")
            print(f"çº¢è‰²é¢„è­¦æ•°é‡: {len(red_warnings)} ({len(red_warnings) / len(self.data) * 100:.2f}%)")

            print(f"\nå½“å‰æµ‹è¯•æ•°æ®è®¡æ•°: {self.test_count}")
            print(f"æµ‹è¯•æ•°æ®åˆ—è¡¨é•¿åº¦: {len(self.test_data_list)}")

            if len(red_warnings) > 0:
                print(f"\nçº¢è‰²é¢„è­¦æ•°æ®æ ·ä¾‹:")
                print(red_warnings.head(10).to_string())

        except Exception as e:
            print(f"è®¡ç®—é¢„è­¦ç»Ÿè®¡æ—¶å‡ºé”™: {e}")


class IntegratedPredictionSystem:
    """é›†æˆçš„é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ"""

    def __init__(self, train_path, test_path):
        self.train_path = train_path
        self.test_path = test_path
        self.regional_model = None
        self.prediction_system = None
        self.anomaly_detector = None
        self.consistency_results = {}

    def initialize_system(self):
        """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
        print("æ­£åœ¨åˆå§‹åŒ–é›†æˆé¢„æµ‹ç³»ç»Ÿ...")

        try:
            # 1. åˆå§‹åŒ–åŒºåŸŸæ¨¡å‹
            self.regional_model = RegionalQuantileModel(self.train_path, self.test_path)
            if self.regional_model.train_data is None:
                print("åŒºåŸŸæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
                return False

            # 2. è®­ç»ƒæ¨¡å‹
            results = self.regional_model.run_analysis()
            if results is None:
                print("æ¨¡å‹è®­ç»ƒå¤±è´¥")
                return False

            # 3. åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ
            self.prediction_system = CasePredictionSystem(self.regional_model)

            # 4. åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰
            self.anomaly_detector = BoxplotAnomalyDetector(
                data_series=self.regional_model.train_df['fee_all']
            )

            if not self.anomaly_detector.is_initialized():
                print("å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥")
                return False

            # 5. è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼å¼‚å¸¸ç±»åˆ«çš„ä¸€è‡´æ€§
            self.calculate_anomaly_consistency()

            print("\nâœ… é›†æˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
            return True

        except Exception as e:
            print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def calculate_anomaly_consistency(self):
        """è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼å¼‚å¸¸ç±»åˆ«çš„ä¸€è‡´æ€§"""
        print("\n=== è®¡ç®—å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§ ===")

        try:
            # è·å–æµ‹è¯•é›†çš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼
            if 0.95 in self.regional_model.models:
                model_info = self.regional_model.models[0.95]
                metrics_all = model_info['metrics'].get('æµ‹è¯•é›†å…¨éƒ¨æ•°æ®', {})

                if 'predictions' in metrics_all and 'actuals' in metrics_all:
                    predictions = metrics_all['predictions']
                    actuals = metrics_all['actuals']

                    # åˆ†ç±»é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¼‚å¸¸ç±»åˆ«
                    pred_categories = [self.anomaly_detector.classify_anomaly_category(pred) for pred in predictions]
                    actual_categories = [self.anomaly_detector.classify_anomaly_category(actual) for actual in actuals]

                    # è¿‡æ»¤æ‰é”™è¯¯åˆ†ç±»
                    valid_indices = [i for i, (p, a) in enumerate(zip(pred_categories, actual_categories))
                                     if p != -1 and a != -1]

                    if len(valid_indices) > 0:
                        pred_categories_valid = [pred_categories[i] for i in valid_indices]
                        actual_categories_valid = [actual_categories[i] for i in valid_indices]

                        # è®¡ç®—å‡†ç¡®ç‡
                        accuracy = accuracy_score(actual_categories_valid, pred_categories_valid)

                        # ç»Ÿè®¡å„ç±»åˆ«åˆ†å¸ƒ
                        pred_dist = Counter(pred_categories_valid)
                        actual_dist = Counter(actual_categories_valid)

                        # ç±»åˆ«æ ‡ç­¾æ˜ å°„
                        category_labels = {
                            0: 'æ­£å¸¸èŒƒå›´',
                            1: 'è½»å¾®è¶…å‡º',
                            2: 'é»„è‰²é¢„è­¦',
                            3: 'æ©™è‰²é¢„è­¦',
                            4: 'çº¢è‰²é¢„è­¦'
                        }

                        print(f"å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")
                        print(f"æœ‰æ•ˆæ ·æœ¬æ•°é‡: {len(valid_indices)}")

                        print("\né¢„æµ‹å€¼å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
                        for category_id, count in sorted(pred_dist.items()):
                            label = category_labels.get(category_id, f'æœªçŸ¥({category_id})')
                            print(f"  {label}: {count}ä¸ª ({count / len(pred_categories_valid) * 100:.2f}%)")

                        print("\nçœŸå®å€¼å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
                        for category_id, count in sorted(actual_dist.items()):
                            label = category_labels.get(category_id, f'æœªçŸ¥({category_id})')
                            print(f"  {label}: {count}ä¸ª ({count / len(actual_categories_valid) * 100:.2f}%)")

                        # ä¿å­˜ç»“æœ
                        self.consistency_results = {
                            'accuracy': accuracy,
                            'pred_dist': pred_dist,
                            'actual_dist': actual_dist,
                            'valid_samples': len(valid_indices)
                        }
                    else:
                        print("æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬å¯ç”¨äºä¸€è‡´æ€§åˆ†æ")
                else:
                    print("æµ‹è¯•é›†é¢„æµ‹ç»“æœä¸å¯ç”¨")
            else:
                print("95%åˆ†ä½æ•°æ¨¡å‹ä¸å¯ç”¨")

        except Exception as e:
            print(f"è®¡ç®—å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§æ—¶å‡ºé”™: {e}")

    def run_interactive_system(self):
        """è¿è¡Œäº¤äº’å¼ç³»ç»Ÿ"""
        if not self.initialize_system():
            return

        print("\n" + "=" * 70)
        print("ğŸ¯ é›†æˆé¢„æµ‹ä¸å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
        print("=" * 70)

        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. å•æ¡ˆä»¶é¢„æµ‹")
            print("2. æŸ¥çœ‹å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡")
            print("3. æŸ¥çœ‹å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§åˆ†æ")
            print("4. é€€å‡ºç³»ç»Ÿ")

            choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()

            if choice == '1':
                self.predict_and_detect()
            elif choice == '2':
                self.show_anomaly_stats()
            elif choice == '3':
                self.show_consistency_analysis()
            elif choice == '4' or choice.lower() in ['quit', 'exit', 'q']:
                print("æ„Ÿè°¢ä½¿ç”¨é›†æˆé¢„æµ‹ç³»ç»Ÿï¼")
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

    def predict_and_detect(self):
        """é¢„æµ‹å¹¶æ£€æµ‹å¼‚å¸¸"""
        try:
            # è·å–é¢„æµ‹ç»“æœ
            predictions, predicted_value = self.prediction_system.predict_case_cost()

            if predictions and predicted_value is not None:

                print("\n" + "=" * 60)
                print("ğŸ” å¼‚å¸¸æ£€æµ‹åˆ†æ")
                print("=" * 60)

                # è¿›è¡Œå¼‚å¸¸æ£€æµ‹
                result, warning_type = self.anomaly_detector.detect_anomaly(predicted_value)

                # æ ¹æ®é¢„è­¦ç±»å‹æ·»åŠ é¢œè‰²æ ‡è¯†
                if warning_type == "red":
                    print(f"ğŸ”´ {result}")
                elif warning_type == "orange":
                    print(f"ğŸŸ  {result}")
                elif warning_type == "yellow":
                    print(f"ğŸŸ¡ {result}")
                elif warning_type == "slight":
                    print(f"ğŸŸ¢ {result}")
                elif warning_type == "normal":
                    print(f"âœ… {result}")
                else:
                    print(f"âŒ {result}")

                # æ˜¾ç¤ºå‚è€ƒèŒƒå›´
                print(f"\nğŸ“Š å‚è€ƒèŒƒå›´:")
                print(f"   æ­£å¸¸èŒƒå›´: â‰¤ {self.anomaly_detector.percentile_95:,.2f}")
                print(
                    f"   é»„è‰²é¢„è­¦: {self.anomaly_detector.percentile_95:,.2f} ~ {self.anomaly_detector.yellow_threshold:,.2f} (10%-30%å¢å¹…)")
                print(
                    f"   æ©™è‰²é¢„è­¦: {self.anomaly_detector.yellow_threshold:,.2f} ~ {self.anomaly_detector.orange_threshold:,.2f} (30%-50%å¢å¹…)")
                print(f"   çº¢è‰²é¢„è­¦: > {self.anomaly_detector.red_threshold:,.2f} (50%ä»¥ä¸Šå¢å¹…)")
                print(f"   å½“å‰æµ‹è¯•è®¡æ•°: {self.anomaly_detector.test_count}")

            else:
                print("âŒ é¢„æµ‹å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹")

        except Exception as e:
            print(f"é¢„æµ‹å’Œæ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    def show_anomaly_stats(self):
        """æ˜¾ç¤ºå¼‚å¸¸æ£€æµ‹ç»Ÿè®¡"""
        if self.anomaly_detector:
            self.anomaly_detector.print_warning_stats()
        else:
            print("å¼‚å¸¸æ£€æµ‹å™¨æœªåˆå§‹åŒ–")

    def show_consistency_analysis(self):
        """æ˜¾ç¤ºå¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§åˆ†æç»“æœ"""
        if self.consistency_results:
            accuracy = self.consistency_results['accuracy']
            pred_dist = self.consistency_results['pred_dist']
            actual_dist = self.consistency_results['actual_dist']
            valid_samples = self.consistency_results['valid_samples']

            print("\n" + "=" * 60)
            print("ğŸ“Š å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§åˆ†æ")
            print("=" * 60)

            print(f"å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")
            print(f"æœ‰æ•ˆæ ·æœ¬æ•°é‡: {valid_samples}")

            # ç±»åˆ«æ ‡ç­¾æ˜ å°„
            category_labels = {
                0: 'æ­£å¸¸èŒƒå›´',
                1: 'è½»å¾®è¶…å‡º',
                2: 'é»„è‰²é¢„è­¦',
                3: 'æ©™è‰²é¢„è­¦',
                4: 'çº¢è‰²é¢„è­¦'
            }

            print("\né¢„æµ‹å€¼å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
            for category_id, count in sorted(pred_dist.items()):
                label = category_labels.get(category_id, f'æœªçŸ¥({category_id})')
                percentage = count / valid_samples * 100
                print(f"  {label}: {count}ä¸ª ({percentage:.2f}%)")

            print("\nçœŸå®å€¼å¼‚å¸¸ç±»åˆ«åˆ†å¸ƒ:")
            for category_id, count in sorted(actual_dist.items()):
                label = category_labels.get(category_id, f'æœªçŸ¥({category_id})')
                percentage = count / valid_samples * 100
                print(f"  {label}: {count}ä¸ª ({percentage:.2f}%)")

            # ä¸šåŠ¡è§£è¯»
            print(f"\nğŸ’¡ ä¸šåŠ¡è§£è¯»:")
            if accuracy > 0.8:
                print("  âœ… æ¨¡å‹åœ¨å¼‚å¸¸ç±»åˆ«è¯†åˆ«ä¸Šè¡¨ç°ä¼˜ç§€")
            elif accuracy > 0.6:
                print("  âœ… æ¨¡å‹åœ¨å¼‚å¸¸ç±»åˆ«è¯†åˆ«ä¸Šè¡¨ç°è‰¯å¥½")
            elif accuracy > 0.4:
                print("  âš ï¸ æ¨¡å‹åœ¨å¼‚å¸¸ç±»åˆ«è¯†åˆ«ä¸Šè¡¨ç°ä¸€èˆ¬")
            else:
                print("  âŒ æ¨¡å‹åœ¨å¼‚å¸¸ç±»åˆ«è¯†åˆ«ä¸Šè¡¨ç°è¾ƒå·®")

        else:
            print("æ²¡æœ‰å¯ç”¨çš„å¼‚å¸¸ç±»åˆ«ä¸€è‡´æ€§åˆ†æç»“æœ")


# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - é›†æˆé¢„æµ‹ç³»ç»Ÿ"""
    print("é›†æˆé¢„æµ‹ä¸å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 50)

    # æ–‡ä»¶è·¯å¾„ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    train_path = "æ¸…æ´—åæ•°æ®.xlsx"  # ä¿®æ”¹ä¸ºæ‚¨çš„è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„
    test_path = "æµ‹è¯•é›†.xlsx"  # ä¿®æ”¹ä¸ºæ‚¨çš„æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„

    try:
        # åˆ›å»ºé›†æˆç³»ç»Ÿå®ä¾‹
        integrated_system = IntegratedPredictionSystem(train_path, test_path)

        # è¿è¡Œäº¤äº’å¼ç³»ç»Ÿ
        integrated_system.run_interactive_system()

    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


